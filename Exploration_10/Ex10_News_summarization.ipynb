{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "declared-chicago",
   "metadata": {},
   "source": [
    "# Exploration 10. News Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-portrait",
   "metadata": {},
   "source": [
    "### 텍스트 요약\n",
    "상대적으로 긴 문장으로 짧은 문장으로 변환하는 기법을 텍스트 요약이라고 합니다. Ex 10에서는 뉴스 요약을 해보겠습니다. \n",
    "\n",
    "텍스트 요약에는 두 가지 방법, Extractive Summarization, Abstractive Summarization이 있습니다. Extractive의 경우 가장 의미있는 문장을 원문 안에서 추출해 내는 방식이고, Abstractive는 원문을 요약하는 새로운 문장을 만들어내는 방식입니다.\n",
    "\n",
    "이 프로젝트에서는 원문과 그에 해당하는 요약문을 주고(지도학습) seq2seq와 Attention을 사용하여 원문 -> 요약문으로의 Abstractive Summarization을 실시하겠습니다. 이는 일반적인 번역과 매우 유사한 과정을 거치는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-mississippi",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-tiffany",
   "metadata": {},
   "source": [
    "전처리 및 모델 학습에 필요한 라이브러리를 import하고, Amazon 리뷰 데이터셋을 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "municipal-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worse-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pleasant-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upGrad learner switches to career in ML & Al w...   \n",
       "1  Delhi techie wins free food from Swiggy for on...   \n",
       "2  New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3  Aegon life iTerm insurance plan helps customer...   \n",
       "4  Have known Hirani for yrs, what if MeToo claim...   \n",
       "\n",
       "                                                text  \n",
       "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
       "1  Kunal Shah's credit card bill payment platform...  \n",
       "2  New Zealand defeated India by 8 wickets in the...  \n",
       "3  With Aegon Life iTerm Insurance plan, customer...  \n",
       "4  Speaking about the sexual harassment allegatio...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-graduate",
   "metadata": {},
   "source": [
    "모델이 text를 입력받아 context vector를 만들어내고, 출력단이 context vector를 통해 headlines와 가까운 문장을 만들어내는 것이 목표입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sixth-arcade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>headlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3867</th>\n",
       "      <td>A Thai man who felt neglected by his in-laws s...</td>\n",
       "      <td>Thai man shoots 6 family members, kills himsel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49919</th>\n",
       "      <td>The Centre has proposed the adoption of the 'S...</td>\n",
       "      <td>Govt proposes 'Singapore Model' to counter rad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95960</th>\n",
       "      <td>MPs played football in Parliament on Wednesday...</td>\n",
       "      <td>MPs play football in Parliament to promote U-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75807</th>\n",
       "      <td>Around 30 children being treated for encephali...</td>\n",
       "      <td>30 children die in 48 hours in UP hospital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32643</th>\n",
       "      <td>The European Union on Thursday launched 'Disco...</td>\n",
       "      <td>European Union invites 15,000 teens to travel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27746</th>\n",
       "      <td>After a video of his son thrashing a man circu...</td>\n",
       "      <td>Don't make mountain out of molehill: MLA on so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14035</th>\n",
       "      <td>China's largest oil and gas producer PetroChin...</td>\n",
       "      <td>PetroChina plans to open first India office in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34118</th>\n",
       "      <td>The Delhi HC has said Airtel's advertisements,...</td>\n",
       "      <td>Airtel disclaimer in IPL ad campaign 'not in b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75297</th>\n",
       "      <td>After a large majority of its population voted...</td>\n",
       "      <td>Sikkim was not part of India for 28 years afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86099</th>\n",
       "      <td>A Toronto couple has accused Uber's delivery s...</td>\n",
       "      <td>Uber accused of sending couple mouldy, half-ea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "3867   A Thai man who felt neglected by his in-laws s...   \n",
       "49919  The Centre has proposed the adoption of the 'S...   \n",
       "95960  MPs played football in Parliament on Wednesday...   \n",
       "75807  Around 30 children being treated for encephali...   \n",
       "32643  The European Union on Thursday launched 'Disco...   \n",
       "27746  After a video of his son thrashing a man circu...   \n",
       "14035  China's largest oil and gas producer PetroChin...   \n",
       "34118  The Delhi HC has said Airtel's advertisements,...   \n",
       "75297  After a large majority of its population voted...   \n",
       "86099  A Toronto couple has accused Uber's delivery s...   \n",
       "\n",
       "                                               headlines  \n",
       "3867   Thai man shoots 6 family members, kills himsel...  \n",
       "49919  Govt proposes 'Singapore Model' to counter rad...  \n",
       "95960  MPs play football in Parliament to promote U-1...  \n",
       "75807         30 children die in 48 hours in UP hospital  \n",
       "32643  European Union invites 15,000 teens to travel ...  \n",
       "27746  Don't make mountain out of molehill: MLA on so...  \n",
       "14035  PetroChina plans to open first India office in...  \n",
       "34118  Airtel disclaimer in IPL ad campaign 'not in b...  \n",
       "75297  Sikkim was not part of India for 28 years afte...  \n",
       "86099  Uber accused of sending couple mouldy, half-ea...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['text', 'headlines']]\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "second-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'text': 'Text', 'headlines': 'Summary'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-pavilion",
   "metadata": {},
   "source": [
    "## preprocess data (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forty-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "# Text의 중복을 제거합니다.\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "native-lafayette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결측치를 확인합니다.\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "macro-dakota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "# 결측치를 제거합니다.\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-budget",
   "metadata": {},
   "source": [
    "## preprocess data (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-helping",
   "metadata": {},
   "source": [
    "자연어 데이터의 문제를 완화하는 전처리를 시도합니다. \"you're\"과  \"you are \"은 같은 말이고, \"너는\"과 \"넌\" 역시 같은 말입니다. 또한, '.........'과 '...............' 을 서로 다른 단어으로 생각할 것인지, '.'라는 단어가 여러 개 나열 된 것은 동일하지만 그 개수에서 차이를 보인다고 생각할 것인지에 대해 명확한 규정이 존재하지 않으므로,\n",
    "어떤 판단기준으로 단어를 선정할 것인지는 엔지니어에 달려 있습니다. \n",
    "모델이 학습할 때 혼란을 덜어줄 것 같은 전처리를 직관적이지만, 가장 효율적인지 알 수 없는(hueristic) 방법으로 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-slovak",
   "metadata": {},
   "source": [
    "[Hueristic - Wikipedia](https://en.wikipedia.org/wiki/Heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "final-samoa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "# 정규화 사전 - 축약어를 분해할 때 사용합니다\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "classical-calgary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# 불용어 사전 - 문맥과 무관하게 자주 나와 context vector에 담겨있는 정보를 희석할 \n",
    "#               염려가 있는 단어는 사용하지 않도록 합니다.\n",
    " \n",
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-church",
   "metadata": {},
   "source": [
    "불용어 사전에서 의문이 드는 점은, 부정어(no, not, nor, but)등도 포함되어 있다는 점인데, 이들은 사용하도록 하겠습니다. 또한, 뉴스에서 의미를 파악하는데 중요할 것 같은 단어도 사용하도록 하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exterior-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = ['but', 'no', 'not', 'nor', 'only', 'while', 'before', 'after', 'under', \\\n",
    "            'again', 'further']\n",
    "\n",
    "my_stopwords = list(\n",
    "                    filter(lambda word: word not in selection,\n",
    "                        stopwords.words('english')\n",
    "                        )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rapid-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자화 불용어 개수:  168\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'or', 'because', 'as', 'until', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('사용자화 불용어 개수: ', len(my_stopwords))\n",
    "print(my_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-punishment",
   "metadata": {},
   "source": [
    "정규표현식과 불용어 사전을 이용해 전처리를 진행합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "indonesian-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(r\"[^a-zA-Z!?.]+\", \" \", sentence) # 영어 외 문자(숫자, 규격 외 특수문자 등) 공백으로 변환\n",
    "    \n",
    "    sentence = re.sub('[m]{3,}', 'm', sentence)  # Ex) ummmmmmm yeah -> um mm yeah\n",
    "    sentence = re.sub('[o]{3,}', 'o', sentence)\n",
    "    \n",
    "    ####\n",
    "    sentence = re.sub(r'[!]{1,}', r' !! ', sentence)\n",
    "    sentence = re.sub(r'[?]{1,}', r' ?? ', sentence)\n",
    "    sentence = re.sub(r'[.]{1,}', r\" .. \", sentence)\n",
    "    \n",
    "    \n",
    "\n",
    "    sentence = re.sub(r'[\" \"]+', r' ', sentence)\n",
    "    \n",
    "\n",
    "    ####\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in my_stopwords if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mighty-cartoon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello .. !! .. name ?? not good ??'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"Helloooooo, ....!........what's your name??????????????  not so good?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "occupational-jersey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything bought great infact ordered twice third ordered wasfor mother father ..\n",
      "great way to start the day !!\n"
     ]
    }
   ],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(preprocess_sentence(temp_text))\n",
    "print(preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-basis",
   "metadata": {},
   "source": [
    "multiprocessing을 이용하여 전체 문장을 전처리합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dressed-albania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.102557182312012  seconds\n",
      "['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience .. program upgrad degree career support helped transition data scientist tech mahindra salary hike .. upgrad online power learning powered lakh careers ..'\n",
      " 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year .. pranav kaushik delhi techie bagged reward after spending cred coins .. users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult .. fit ..'\n",
      " 'new zealand defeated india wickets fourth odi hamilton thursday win first match five match odi series .. india lost international match under rohit sharma captaincy after consecutive victories dating back march .. match witnessed india getting seventh lowest total odi cricket history ..'\n",
      " ...\n",
      " 'according reports new version science fiction film matrix development .. michael jordan reportedly play lead role film .. screenwriter zak penn talks write script film reports added .. actor keanu reeves starred original film followed two sequels ..'\n",
      " 'new music video shows rapper snoop dogg aiming toy gun clown character parodying us president donald trump .. video also shows tv airing news conference headline ronald klump wants deport doggs airing live clown house .. video remixed version song lavender ..'\n",
      " 'madhesi morcha alliance seven political parties withdrawn support pm pushpa kamal dahal led nepal government after failed meet seven day ultimatum fulfil demands including endorsement revised constitution amendment bill .. morcha seats parliament but despite withdrawal support no immediate threat government ..']\n",
      "6.182835340499878  seconds\n",
      "['upgrad learner switches to career in ml al with salary hike'\n",
      " 'delhi techie wins free food from swiggy for one year on cred'\n",
      " 'new zealand end rohit sharma led india match winning streak' ...\n",
      " 'the matrix film to get reboot reports'\n",
      " 'snoop dogg aims gun at clown dressed as trump in new video'\n",
      " 'madhesi morcha withdraws support to nepalese government']\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp   # 멀티 프로세싱으로 전처리 속도를 획기적으로 줄여봅시다\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial  # map을 할 때 함수에 여러 인자를 넣어줄 수 있도록 합니다\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# num_cores 만큼 쪼개진 데이터를 전처리하여 반환합니다\n",
    "def appendTexts(sentences, remove_stopwords):\n",
    "    texts = []\n",
    "    for s in sentences:\n",
    "        texts += preprocess_sentence(s, remove_stopwords),\n",
    "    return texts\n",
    "\n",
    "def preprocess_data(data, remove_stopwords=True):\n",
    "    start_time = time.time()\n",
    "    num_cores = mp.cpu_count()  # 컴퓨터의 코어 수를 구합니다\n",
    "\n",
    "    text_data_split = np.array_split(data, num_cores)  # 코어 수만큼 데이터를 배분하여 병렬적으로 처리할 수 있게 합니다\n",
    "    pool = Pool(num_cores)\n",
    "\n",
    "    processed_data = np.concatenate(pool.map(partial(appendTexts, remove_stopwords=remove_stopwords), text_data_split))  # 각자 작업한 데이터를 하나로 합쳐줍니다\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(time.time() - start_time, \" seconds\")\n",
    "    return processed_data\n",
    "\n",
    "clean_text = preprocess_data(data['Text'])  # 클라우드 기준으로 3~4분 정도 소요 됩니다\n",
    "print(clean_text)\n",
    "\n",
    "clean_summary = preprocess_data(data['Summary'], remove_stopwords=False) # 클라우드 기준 1분정도 소요됩니다.\n",
    "print(clean_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "experimental-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame으로 저장합니다\n",
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "authorized-lighting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text       0\n",
       "Summary    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인 74개의 빈 summary 확인되었습니다\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "legal-treasurer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 93154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 결측치 제거\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-vietnamese",
   "metadata": {},
   "source": [
    "자연어 관련 전처리가 완료되었습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-cream",
   "metadata": {},
   "source": [
    "## preprocess data (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-birmingham",
   "metadata": {},
   "source": [
    "인공지능 모델에게 이 데이터를 먹이려면, 자연어 형태가 아닌 정수로 이루어진 numpy ndarray 형태로 바꾸어 주어야 합니다. ndarray는 모든 행의 차원(길이)가 같아야 하므로, 긴 문장이든 짧은 문장이든 특정한 길이로 바꿔주어야 합니다. 이때,  긴 문장의 경우 제외를 할 수도 있고, 앞이나 뒤를 잘라 특정 길이로 바꿔 줄 수도 있습니다. 짧은 문장의 경우에도, 제외를 하거나 앞 뒤에 의미없는 표시(padding)을 넣어줍니다\n",
    "\n",
    "단어로 분해 후에는 각 단어를 출현 빈도순으로 정리하여 {단어: 출현 빈도 순위} 인 딕셔너리와 {출현 빈도 순위: 단어}인 딕셔너리를 만듭니다. 이때 출현한 모든 단어를 저장하는 대신, '사전'의 길이를 조정합니다. 이를 통해서 분해된 문장을 정수로 인코딩할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "presidential-hello",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 1\n",
      "텍스트의 최대 길이 : 67\n",
      "텍스트의 평균 길이 : 39.22306832045547\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 16\n",
      "요약의 평균 길이 : 9.340270435136235\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 100)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-flush",
   "metadata": {},
   "source": [
    "데이터의 분포가 넓게 퍼져있음을 알 수 있습니다. 이 프로젝트의 목표는 긍정/부정 분석이 아니고, 뉴스를 요약하는 것이며, 짧은 Text의 경우, padding을 pre로 주게 된다면, 해당 단어가 연이은 padding 뒤에 나와 그에 연관된 context vector가 희석 없이 decoder로 넘어가게 되어 오히려 너무 강한 영향력을 행사할 것 같습니다. 따라서 짧은 텍스트에서 얻을 수 있는 정보는 모두 긴 Text에서 충분히 얻을 수 있다고 가정하고 너무 짧거나 긴 Text를 갖는 데이터포인트를 제거하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ethical-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최소, 최대 사이에 있는 샘플을 골라 냅니다.\n",
    "def bandpass_len(min_len, max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s.split()) <= max_len) and (len(s.split()) >= min_len):\n",
    "            cnt = cnt + 1\n",
    "    print(f'전체 샘플 중 길이가 {min_len} 이상,{max_len} 이하인 샘플의 비율:{cnt / len(nested_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "innovative-personality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 1 이상,29 이하인 샘플의 비율:0.004747864985766572\n"
     ]
    }
   ],
   "source": [
    "bandpass_len(1, 29, data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "powered-malawi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 30 이상,46 이하인 샘플의 비율:0.9668971126474176\n"
     ]
    }
   ],
   "source": [
    "bandpass_len(30, 46, data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "blocked-component",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 47 이상,3000 이하인 샘플의 비율:0.02835502236681578\n"
     ]
    }
   ],
   "source": [
    "bandpass_len(47, 3000, data['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-stand",
   "metadata": {},
   "source": [
    "\\[30, 46\\]의 구간만 사용하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hybrid-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 46; text_min_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "governmental-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len \\\n",
    "            and len(x.split()) >= text_min_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "general-slovenia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saurav kant alumnus upgrad iiit pg program mac...</td>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>speaking sexual harassment allegations rajkuma...</td>\n",
       "      <td>have known hirani for yrs what if metoo claims...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pakistani singer rahat fateh ali khan denied r...</td>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98396</th>\n",
       "      <td>crpf jawan tuesday axed death sharp edged weap...</td>\n",
       "      <td>crpf jawan axed to death by maoists in chhatti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98397</th>\n",
       "      <td>uff yeh first song sonakshi sinha starrer upco...</td>\n",
       "      <td>first song from sonakshi sinha noor titled uff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98398</th>\n",
       "      <td>according reports new version science fiction ...</td>\n",
       "      <td>the matrix film to get reboot reports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98399</th>\n",
       "      <td>new music video shows rapper snoop dogg aiming...</td>\n",
       "      <td>snoop dogg aims gun at clown dressed as trump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98400</th>\n",
       "      <td>madhesi morcha alliance seven political partie...</td>\n",
       "      <td>madhesi morcha withdraws support to nepalese g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95104 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "0      saurav kant alumnus upgrad iiit pg program mac...   \n",
       "2      new zealand defeated india wickets fourth odi ...   \n",
       "3      aegon life iterm insurance plan customers enjo...   \n",
       "4      speaking sexual harassment allegations rajkuma...   \n",
       "5      pakistani singer rahat fateh ali khan denied r...   \n",
       "...                                                  ...   \n",
       "98396  crpf jawan tuesday axed death sharp edged weap...   \n",
       "98397  uff yeh first song sonakshi sinha starrer upco...   \n",
       "98398  according reports new version science fiction ...   \n",
       "98399  new music video shows rapper snoop dogg aiming...   \n",
       "98400  madhesi morcha alliance seven political partie...   \n",
       "\n",
       "                                                 Summary  \n",
       "0      upgrad learner switches to career in ml al wit...  \n",
       "2      new zealand end rohit sharma led india match w...  \n",
       "3      aegon life iterm insurance plan helps customer...  \n",
       "4      have known hirani for yrs what if metoo claims...  \n",
       "5      rahat fateh ali khan denies getting notice for...  \n",
       "...                                                  ...  \n",
       "98396  crpf jawan axed to death by maoists in chhatti...  \n",
       "98397  first song from sonakshi sinha noor titled uff...  \n",
       "98398              the matrix film to get reboot reports  \n",
       "98399  snoop dogg aims gun at clown dressed as trump ...  \n",
       "98400  madhesi morcha withdraws support to nepalese g...  \n",
       "\n",
       "[95104 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-green",
   "metadata": {},
   "source": [
    "Summary의 분포도 살펴보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "twenty-increase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 7 이상,13 이하인 샘플의 비율:0.979496130551817\n"
     ]
    }
   ],
   "source": [
    "bandpass_len(7, 13, data['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "downtown-railway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22               killed injured in saudi arabia floods\n",
       "37              pakistan holds keys to war afghanistan\n",
       "77               swine flu outbreak kills in rajasthan\n",
       "182         confiscated benami assets worth crore dept\n",
       "196               nana patekar mother passes away aged\n",
       "                             ...                      \n",
       "98093         taliban made crore selling pistachios in\n",
       "98098              year old homeless man gets makeover\n",
       "98250    union cabinet approves national health policy\n",
       "98324     trailer of baahubali the conclusion released\n",
       "98327               ranchi becomes india th test venue\n",
       "Name: Summary, Length: 1739, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Summary'][data['Summary'].apply(lambda x: len(x.split())<=6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-corporation",
   "metadata": {},
   "source": [
    "한 단어로 이루어진 summary는 삭제하고, 10 이하의 샘플만 사용하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "polar-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_max_len = 13; summary_min_len = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "amino-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len \\\n",
    "            and len(x.split()) >= summary_min_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bulgarian-absorption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saurav kant alumnus upgrad iiit pg program mac...</td>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>speaking sexual harassment allegations rajkuma...</td>\n",
       "      <td>have known hirani for yrs what if metoo claims...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pakistani singer rahat fateh ali khan denied r...</td>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98396</th>\n",
       "      <td>crpf jawan tuesday axed death sharp edged weap...</td>\n",
       "      <td>crpf jawan axed to death by maoists in chhatti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98397</th>\n",
       "      <td>uff yeh first song sonakshi sinha starrer upco...</td>\n",
       "      <td>first song from sonakshi sinha noor titled uff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98398</th>\n",
       "      <td>according reports new version science fiction ...</td>\n",
       "      <td>the matrix film to get reboot reports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98399</th>\n",
       "      <td>new music video shows rapper snoop dogg aiming...</td>\n",
       "      <td>snoop dogg aims gun at clown dressed as trump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98400</th>\n",
       "      <td>madhesi morcha alliance seven political partie...</td>\n",
       "      <td>madhesi morcha withdraws support to nepalese g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "0      saurav kant alumnus upgrad iiit pg program mac...   \n",
       "2      new zealand defeated india wickets fourth odi ...   \n",
       "3      aegon life iterm insurance plan customers enjo...   \n",
       "4      speaking sexual harassment allegations rajkuma...   \n",
       "5      pakistani singer rahat fateh ali khan denied r...   \n",
       "...                                                  ...   \n",
       "98396  crpf jawan tuesday axed death sharp edged weap...   \n",
       "98397  uff yeh first song sonakshi sinha starrer upco...   \n",
       "98398  according reports new version science fiction ...   \n",
       "98399  new music video shows rapper snoop dogg aiming...   \n",
       "98400  madhesi morcha alliance seven political partie...   \n",
       "\n",
       "                                                 Summary  \n",
       "0      upgrad learner switches to career in ml al wit...  \n",
       "2      new zealand end rohit sharma led india match w...  \n",
       "3      aegon life iterm insurance plan helps customer...  \n",
       "4      have known hirani for yrs what if metoo claims...  \n",
       "5      rahat fateh ali khan denies getting notice for...  \n",
       "...                                                  ...  \n",
       "98396  crpf jawan axed to death by maoists in chhatti...  \n",
       "98397  first song from sonakshi sinha noor titled uff...  \n",
       "98398              the matrix film to get reboot reports  \n",
       "98399  snoop dogg aims gun at clown dressed as trump ...  \n",
       "98400  madhesi morcha withdraws support to nepalese g...  \n",
       "\n",
       "[93154 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "coordinate-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saurav kant alumnus upgrad iiit pg program mac...</td>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "      <td>sostoken upgrad learner switches to career in ...</td>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "      <td>sostoken new zealand end rohit sharma led indi...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "      <td>sostoken aegon life iterm insurance plan helps...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>speaking sexual harassment allegations rajkuma...</td>\n",
       "      <td>have known hirani for yrs what if metoo claims...</td>\n",
       "      <td>sostoken have known hirani for yrs what if met...</td>\n",
       "      <td>have known hirani for yrs what if metoo claims...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pakistani singer rahat fateh ali khan denied r...</td>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "      <td>sostoken rahat fateh ali khan denies getting n...</td>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  saurav kant alumnus upgrad iiit pg program mac...   \n",
       "2  new zealand defeated india wickets fourth odi ...   \n",
       "3  aegon life iterm insurance plan customers enjo...   \n",
       "4  speaking sexual harassment allegations rajkuma...   \n",
       "5  pakistani singer rahat fateh ali khan denied r...   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  upgrad learner switches to career in ml al wit...   \n",
       "2  new zealand end rohit sharma led india match w...   \n",
       "3  aegon life iterm insurance plan helps customer...   \n",
       "4  have known hirani for yrs what if metoo claims...   \n",
       "5  rahat fateh ali khan denies getting notice for...   \n",
       "\n",
       "                                       decoder_input  \\\n",
       "0  sostoken upgrad learner switches to career in ...   \n",
       "2  sostoken new zealand end rohit sharma led indi...   \n",
       "3  sostoken aegon life iterm insurance plan helps...   \n",
       "4  sostoken have known hirani for yrs what if met...   \n",
       "5  sostoken rahat fateh ali khan denies getting n...   \n",
       "\n",
       "                                      decoder_target  \n",
       "0  upgrad learner switches to career in ml al wit...  \n",
       "2  new zealand end rohit sharma led india match w...  \n",
       "3  aegon life iterm insurance plan helps customer...  \n",
       "4  have known hirani for yrs what if metoo claims...  \n",
       "5  rahat fateh ali khan denies getting notice for...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "literary-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array로 저장\n",
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "charged-logic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88990   117 27494 ... 15996  8011 12116]\n",
      "테스트 데이터의 수 : 18630\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dated-professional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 74524\n",
      "훈련 레이블의 개수 : 74524\n",
      "테스트 데이터의 개수 : 18630\n",
      "테스트 레이블의 개수 : 18630\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-lucas",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cleared-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "personal-railway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 67077\n",
      "등장 빈도가 12번 이하인 희귀 단어의 수: 51598\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 15479\n",
      "단어 집합에서 희귀 단어의 비율: 76.9235356381472\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.488910962574639\n"
     ]
    }
   ],
   "source": [
    "threshold = 13\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "varied-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 15478\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 15476+2으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "worldwide-retirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[418, 14, 1970, 6098, 5652, 1105, 835, 276, 7842, 1275, 1847, 23, 208, 79, 788, 252, 409, 5, 16, 752, 6635, 9321, 208, 312, 532, 646, 293, 23, 208, 607, 28, 76, 9321], [3314, 6806, 1072, 4314, 1281, 854, 383, 199, 1519, 5903, 2615, 688, 8107, 12, 86, 1524, 318, 199, 1258, 5, 16, 9911, 926, 381, 58, 774, 331, 1331, 11117, 1388], [283, 51, 18, 3027, 2810, 1, 3, 2721, 173, 11441, 13, 1, 7078, 39, 71, 2, 2810, 384, 156, 3, 7078, 24, 13, 283, 934, 13, 3, 1895, 8513, 42, 2840]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-technology",
   "metadata": {},
   "source": [
    "summary에 대해서는 decoder_input으로 토큰화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "arranged-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "directed-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 29218\n",
      "등장 빈도가 10번 이하인 희귀 단어의 수: 22209\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 7009\n",
      "단어 집합에서 희귀 단어의 비율: 76.01136285851187\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 7.849736868377141\n"
     ]
    }
   ],
   "source": [
    "threshold = 11\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "environmental-plane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 4356, 1252, 528, 2750, 2670, 4, 43, 217, 1422], [1, 704, 4236, 2555, 966, 3, 6276, 189], [1, 153, 10, 2937, 3, 337, 34, 7, 141, 4100, 238], [1, 109, 159, 110, 352, 16, 45, 1135, 4357], [1, 71, 52, 11, 320, 4, 1402, 376, 29, 110]]\n",
      "target\n",
      "decoder  [[4356, 1252, 528, 2750, 2670, 4, 43, 217, 1422, 2], [704, 4236, 2555, 966, 3, 6276, 189, 2], [153, 10, 2937, 3, 337, 34, 7, 141, 4100, 238, 2], [109, 159, 110, 352, 16, 45, 1135, 4357, 2], [71, 52, 11, 320, 4, 1402, 376, 29, 110, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 7011\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-anderson",
   "metadata": {},
   "source": [
    "sostoken과 eostoken이 각각 1, 2로 인코딩된 것과 변환이 제대로 되었음을 확인했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "equivalent-lodging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 0\n",
      "삭제할 테스트 데이터의 개수 : 0\n",
      "훈련 데이터의 개수 : 74524\n",
      "훈련 레이블의 개수 : 74524\n",
      "테스트 데이터의 개수 : 18630\n",
      "테스트 레이블의 개수 : 18630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# 결측치 제거 - eostoken만 남은 경우를 제거한다. 아마 없을 것이다\n",
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-bubble",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-brief",
   "metadata": {},
   "source": [
    "encoder의 경우, padding을 뒤에 넣으면 context vector가 희석되므로 앞에 padding을 넣습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "large-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='pre')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='pre')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-round",
   "metadata": {},
   "source": [
    "# Modeling & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "another-reporter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "lined-duncan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "broken-infrastructure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 46)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 46, 128)      1981184     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 46, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 46, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    897408      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 46, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7011)   1801827     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,519,523\n",
      "Trainable params: 6,519,523\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model2 = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "structural-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "removable-rough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 46)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 46, 128)      1981184     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 46, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 46, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    897408      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 46, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 7011)   3596643     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 8,445,667\n",
      "Trainable params: 8,445,667\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model2 = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "composite-thousand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "292/292 [==============================] - 214s 695ms/step - loss: 5.3847 - val_loss: 4.7588\n",
      "Epoch 2/50\n",
      "292/292 [==============================] - 207s 709ms/step - loss: 4.7231 - val_loss: 4.4621\n",
      "Epoch 3/50\n",
      "292/292 [==============================] - 207s 709ms/step - loss: 4.3720 - val_loss: 4.1541\n",
      "Epoch 4/50\n",
      "292/292 [==============================] - 205s 704ms/step - loss: 4.0655 - val_loss: 3.9450\n",
      "Epoch 5/50\n",
      "292/292 [==============================] - 207s 710ms/step - loss: 3.8285 - val_loss: 3.7956\n",
      "Epoch 6/50\n",
      "292/292 [==============================] - 208s 714ms/step - loss: 3.6541 - val_loss: 3.6780\n",
      "Epoch 7/50\n",
      "292/292 [==============================] - 207s 709ms/step - loss: 3.5019 - val_loss: 3.5909\n",
      "Epoch 8/50\n",
      "292/292 [==============================] - 207s 710ms/step - loss: 3.3813 - val_loss: 3.5035\n",
      "Epoch 9/50\n",
      "292/292 [==============================] - 208s 712ms/step - loss: 3.2679 - val_loss: 3.4447\n",
      "Epoch 10/50\n",
      "292/292 [==============================] - 207s 710ms/step - loss: 3.1732 - val_loss: 3.4051\n",
      "Epoch 11/50\n",
      "292/292 [==============================] - 207s 711ms/step - loss: 3.0806 - val_loss: 3.3730\n",
      "Epoch 12/50\n",
      "292/292 [==============================] - 206s 707ms/step - loss: 3.0068 - val_loss: 3.3217\n",
      "Epoch 13/50\n",
      "292/292 [==============================] - 208s 711ms/step - loss: 2.9357 - val_loss: 3.2807\n",
      "Epoch 14/50\n",
      "292/292 [==============================] - 206s 707ms/step - loss: 2.8660 - val_loss: 3.2572\n",
      "Epoch 15/50\n",
      "292/292 [==============================] - 208s 713ms/step - loss: 2.8110 - val_loss: 3.2357\n",
      "Epoch 16/50\n",
      "292/292 [==============================] - 208s 712ms/step - loss: 2.7532 - val_loss: 3.2329\n",
      "Epoch 17/50\n",
      "292/292 [==============================] - 208s 711ms/step - loss: 2.7086 - val_loss: 3.2157\n",
      "Epoch 18/50\n",
      "292/292 [==============================] - 207s 708ms/step - loss: 2.6562 - val_loss: 3.2007\n",
      "Epoch 19/50\n",
      "292/292 [==============================] - 208s 714ms/step - loss: 2.6210 - val_loss: 3.1776\n",
      "Epoch 20/50\n",
      "292/292 [==============================] - 209s 715ms/step - loss: 2.5720 - val_loss: 3.1745\n",
      "Epoch 21/50\n",
      "292/292 [==============================] - 208s 713ms/step - loss: 2.5346 - val_loss: 3.1696\n",
      "Epoch 22/50\n",
      "292/292 [==============================] - 209s 716ms/step - loss: 2.4949 - val_loss: 3.1686\n",
      "Epoch 23/50\n",
      "292/292 [==============================] - 208s 712ms/step - loss: 2.4626 - val_loss: 3.1523\n",
      "Epoch 24/50\n",
      "292/292 [==============================] - 211s 722ms/step - loss: 2.4298 - val_loss: 3.1509\n",
      "Epoch 25/50\n",
      "292/292 [==============================] - 209s 717ms/step - loss: 2.4036 - val_loss: 3.1579\n",
      "Epoch 26/50\n",
      "292/292 [==============================] - 207s 710ms/step - loss: 2.3718 - val_loss: 3.1612\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model2.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "frank-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_weights('news_summarizer_weights_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "nearby-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.load_weights('summarizer_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "united-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "taken-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "younger-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "mental-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "opposite-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "complex-workplace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 12160\n",
      "원문 : tamil nadu farmers monday ran naked near pm narendra modi office demanding crore drought relief fund centre reportedly comes after promised appointment modi but not allowed meet later farmers sitting protest since almost month holding skulls hands live mice mouths \n",
      "실제 요약 : tn farmers protest naked near pmo seeking drought relief \n",
      "예측 요약 :  tamil nadu cm announces pm modi worth cr on pm\n",
      "\n",
      "\n",
      "id: 12161\n",
      "원문 : donald trump saturday completed first days us president after taking oath office january days trump issued dozens executive orders including immigration ban citizens six muslim majority countries trump also warned north korea major conflict nuclear missile programmes weeks time \n",
      "실제 요약 : in pics trump completes first days as us president \n",
      "예측 요약 :  trump st to visit us open to us prez\n",
      "\n",
      "\n",
      "id: 12162\n",
      "원문 : lawsuit claimed american audio equipment company bose spying users wireless headphones app tracks music listen bose violates users privacy rights selling audio information without permission added audio choices offer incredible amount insight customer personality behaviour religion lawsuit said \n",
      "실제 요약 : bose wireless spying on customers claims lawsuit \n",
      "예측 요약 :  sues app for hacking users in\n",
      "\n",
      "\n",
      "id: 12163\n",
      "원문 : following lalu prasad yadav sentenced years jail fodder scam case son tejashwi yadav said approach high court after studying sentence apply bail meanwhile son tej pratap yadav said confident get bail full faith judiciary \n",
      "실제 요약 : we will apply for bail in hc tejashwi on lalu sentence \n",
      "예측 요약 :  lalu son son granted bail in years in jail\n",
      "\n",
      "\n",
      "id: 12164\n",
      "원문 : bangladesh defeated sri lanka runs series friday achieving biggest margin victory runs odi cricket loss sri lanka fourth consecutive th last odis bangladesh rounder shakib al hasan named man match taking three wickets scoring runs \n",
      "실제 요약 : desh post their biggest win as sl lose fourth straight odi \n",
      "예측 요약 :  sl beat sl to win their th odi series in series\n",
      "\n",
      "\n",
      "id: 12165\n",
      "원문 : following threats us international criminal court said continue investigate war crimes court independent judicial institution added us warned icc consequences proceeds opening investigation alleged war crimes committed us military intelligence staff afghanistan war \n",
      "실제 요약 : icc says will continue after us threats \n",
      "예측 요약 :  us court upholds cbi to un in ayodhya case\n",
      "\n",
      "\n",
      "id: 12166\n",
      "원문 : berkshire hathaway vice chairman charlie while speaking company annual meeting said firm chairman billionaire warren buffett version mahatma gandhi further said according berkshire one big advantage lot people trying brilliant trying stay \n",
      "실제 요약 : buffett version of mahatma gandhi business partner \n",
      "예측 요약 :  buffett stock is not buffett\n",
      "\n",
      "\n",
      "id: 12167\n",
      "원문 : dot called dot word known combination words tiny little refers small stroke dot writing printing derived latin word aims distinguish letters vertical \n",
      "실제 요약 : what are the over and called \n",
      "예측 요약 :  twitter reacts to logo\n",
      "\n",
      "\n",
      "id: 12168\n",
      "원문 : sacked aap minister kapil mishra sunday accused aam aadmi party involved big donation scam alleging party concealed actual amount donations used shell companies black money kejriwal associates part scam said also approach cbi documents \n",
      "실제 요약 : aap used fake firms to black money sacked aap min \n",
      "예측 요약 :  aap mishra mishra booked for aap mishra\n",
      "\n",
      "\n",
      "id: 12169\n",
      "원문 : after former mp jaya compared alauddin khilji character film padmaavat samajwadi party leader azam khan called wali concentrate politics pay attention remarks khan asked earlier said khilji character reminded khan harassed election contested \n",
      "실제 요약 : azam khan calls jaya over khilji remark \n",
      "예측 요약 :  hai song for deepika on padmavati\n",
      "\n",
      "\n",
      "id: 12170\n",
      "원문 : video assistant sub inspector punjab thrashing elderly woman dragging hair state district surfaced online woman brick labourer protesting owner reportedly not paying wages according government rates policeman also threw ground trying stop \n",
      "실제 요약 : cop beats elderly woman her by her hair in punjab \n",
      "예측 요약 :  cop thrashes woman for misbehaving with her in assam\n",
      "\n",
      "\n",
      "id: 12171\n",
      "원문 : civilian jammu filed police complaint after picture circulated electronic print media one six lashkar taiba terrorists killed security forces saturday demanding probe incident said somebody downloaded picture facebook account circulated one dead terrorists \n",
      "실제 요약 : man finds his pic among dead militants files complaint \n",
      "예측 요약 :  journo suspended over photo of gorkhaland encounter\n",
      "\n",
      "\n",
      "id: 12172\n",
      "원문 : man kaur year old woman chandigarh completed metre sprint world masters games auckland monday taking one minute seconds finish race kaur only competitor category thereby awarded gold medal effort kaur started running aged also compete shot put javelin \n",
      "실제 요약 : yr old indian finishes race at international event \n",
      "예측 요약 :  year old woman breaks world record in sets\n",
      "\n",
      "\n",
      "id: 12173\n",
      "원문 : stephen hawking penned joint letter protest article challenged big bang theory article gave alternate big bounce theory stating universe bounced back collapsing state current expanding state contract again physicists saying big bang main describing universe birth \n",
      "실제 요약 : hawking others write against theory challenging big bang \n",
      "예측 요약 :  einstein theory theory theory theory big on big bang theory\n",
      "\n",
      "\n",
      "id: 12174\n",
      "원문 : under adopt lion campaign new zealand rugby fans offered free stay visiting irish british rugby fans expected fly new zealand june rugby tour visitors expected while new offered free accommodation many also offered pick visitors airports \n",
      "실제 요약 : nz rugby fans offer to host british visitors for free \n",
      "예측 요약 :  zealand fans protest red card for free red card in nz\n",
      "\n",
      "\n",
      "id: 12175\n",
      "원문 : landfill site gujarat vadodara converted tree museum after municipal corporation planted saplings nearly varieties area square metres site reportedly worth crore began after locals complained bad smell emanating landfill \n",
      "실제 요약 : site in vadodara converted into tree museum \n",
      "예측 요약 :  gujarat village finds kg of trees\n",
      "\n",
      "\n",
      "id: 12176\n",
      "원문 : official twitter handle mumbai police shared meme amitabh bachchan aamir khan starrer thugs hindostan meme features dialogue used aamir film trailer no place thugs mumbai read caption post reacting post twitter user commented mumbai police \n",
      "실제 요약 : no place for thugs in mumbai mumbai police on thugs of hindostan \n",
      "예측 요약 :  police shares thugs of thugs of hindostan in thugs of hindostan\n",
      "\n",
      "\n",
      "id: 12177\n",
      "원문 : many seven people killed accident uttar pradesh ghaziabad friday after child accidentally suv parked vehicle carrying wedding party around people rolled gorge near highway groom father among killed \n",
      "실제 요약 : killed in ghaziabad after child car \n",
      "예측 요약 :  dead as bus falls off gorge in bihar\n",
      "\n",
      "\n",
      "id: 12178\n",
      "원문 : former judge raghav allegedly opened fire haryana electricity board workers while fixing power transmission line outside home civil lines gurugram thursday raghav reportedly frequent power cuts area fired shots air four towards workers police official said \n",
      "실제 요약 : ex judge shoots at power line workers over outage \n",
      "예측 요약 :  ex judge liquor shops in gurugram fire\n",
      "\n",
      "\n",
      "id: 12179\n",
      "원문 : carlos award winning south african filmmaker died age after head safari lodge south africa working feature film lodge mishap occurred flown helicopter hospital johannesburg later succumbed injuries \n",
      "실제 요약 : south african filmmaker dies on being head by \n",
      "예측 요약 :  dies after death title\n",
      "\n",
      "\n",
      "id: 12180\n",
      "원문 : year old maharashtra man whose genitals chopped three people including year old woman stalking died deceased reportedly gone woman house told husband before incident woman warned dire consequences not stop stalking \n",
      "실제 요약 : stalker dies after woman chopped off his genitals in maharashtra \n",
      "예측 요약 :  man stabs neighbour after being raped by husband in maha\n",
      "\n",
      "\n",
      "id: 12181\n",
      "원문 : actress sonali bendre sister law arya said sonali diagnosed cancer currently undergoing treatment new york staying strong july sonali revealed diagnosed cancer sonali earlier shared year old son ranveer took news cancer diagnosis \n",
      "실제 요약 : sonali is strong sonali kin on cancer treatment \n",
      "예측 요약 :  sonali reveals she was cancer in cancer treatment\n",
      "\n",
      "\n",
      "id: 12182\n",
      "원문 : shekhar kapur set direct little dragon authorised biopic legendary artist actor bruce lee late actor chinese screen name li means little dragon film contemporary take bruce lee important audiences today relate lives journey bruce lee said kapur \n",
      "실제 요약 : to direct lee biopic little \n",
      "예측 요약 :  rajkummar rao to play in biopic\n",
      "\n",
      "\n",
      "id: 12183\n",
      "원문 : windies rounder chris gayle returned odi cricket after gap days after taking field first england windies odi tuesday gayle last odi match new zealand icc world cup quarter final year old national team due contract dispute board cricketers \n",
      "실제 요약 : chris gayle returns to odi cricket after days \n",
      "예측 요약 :  chris gayle hits his first odi ton in south africa\n",
      "\n",
      "\n",
      "id: 12184\n",
      "원문 : uttar pradesh oldest prisoner year old yadav released thursday after serving years jail year old murder case freed recommendation previous samajwadi party government governor ram naik using constitutional powers yadav release proposed sp government january \n",
      "실제 요약 : year old uttar pradesh oldest prisoner freed from jail \n",
      "예측 요약 :  year old dies in days after death of rapists\n",
      "\n",
      "\n",
      "id: 12185\n",
      "원문 : royal caribbean international refunded several passengers board voyager seas cruise ship singapore australia after complaints around indians working filed passengers said indian men harassed female passengers recording brought dancers also blocked number ship amenities passengers claimed \n",
      "실제 요약 : staff on cruise passengers \n",
      "예측 요약 :  airline to give back to crew for flight\n",
      "\n",
      "\n",
      "id: 12186\n",
      "원문 : us president donald trump said mass shooting killed least people texas church sunday caused due attacker mental health problem gun ownership not factor added notably people killed us year incidents gun violence \n",
      "실제 요약 : mental issue not gun caused texas shooting trump \n",
      "예측 요약 :  trump says he is not dead in shooting us prez\n",
      "\n",
      "\n",
      "id: 12187\n",
      "원문 : hearing privacy policy whatsapp centre told supreme court friday users data connected personality integral right life guaranteed under constitution further added government soon implement regulatory regime data protection \n",
      "실제 요약 : data protection to right to life centre tells sc \n",
      "예측 요약 :  sc allows whatsapp to privacy privacy verdict\n",
      "\n",
      "\n",
      "id: 12188\n",
      "원문 : sri lankan president maithripala sirisena sunday announced nationwide state emergency lifted emergency imposed control communal violence after muslims clashed country central district two people killed hundreds muslim owned properties mosques damaged according reports \n",
      "실제 요약 : sri lanka lifts nationwide emergency after days \n",
      "예측 요약 :  sri lanka prez blocks sri lanka amid floods\n",
      "\n",
      "\n",
      "id: 12189\n",
      "원문 : senior bjp leader subramanian swamy announced construction ram temple ayodhya begin soon devotees able celebrate diwali temple next year everything ready materials construction pre fabricated matter attaching like swami narayan mandir swamy added \n",
      "실제 요약 : next diwali will be celebrated in ram temple swamy \n",
      "예측 요약 :  ram mandir construction will be built in ayodhya issue swamy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(12160, 12190):\n",
    "    print('id:', i)\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "endless-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_expectation(idx):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[idx]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[idx]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[idx].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "humanitarian-summer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : nasa astronaut ricky arnold shared selfie taken spacewalk outside international space station orbits earth km altitude arnold fellow drew worked six hours swap malfunctioning cooling box nicknamed source leak five years ago another relatively box called \n",
      "실제 요약 : astronaut shares selfie taken above earth \n",
      "예측 요약 :  astronaut shares view from space station\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_expectation(12040)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-suicide",
   "metadata": {},
   "source": [
    "아직 의미를 잘 파악하지 못 하는 것 같습니다. 또한 요약된 문장을 살펴보면 주어와 목적어를 같게 한 경우가 많은데, 해결이 필요합니다. 또한, Tokenize과정에서 마침표가 생략되어, 그것도 넣어주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-margin",
   "metadata": {},
   "source": [
    "# Trial 2\n",
    "summary의 길이를 강하게 한정하여 의미없는 동어반복이 일어난 것 같습니다. 또한 tokenize의 filter를 수정하여 같은 모델로 학습해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-occurrence",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "acting-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "data2 = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "featured-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2[['text', 'headlines']]\n",
    "data2.rename(columns = {'text': 'Text', 'headlines': 'Summary'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-hawaii",
   "metadata": {},
   "source": [
    "## preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "surgical-radar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.161367654800415  seconds\n",
      "['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience .. program upgrad degree career support helped transition data scientist tech mahindra salary hike .. upgrad online power learning powered lakh careers ..'\n",
      " 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year .. pranav kaushik delhi techie bagged reward after spending cred coins .. users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult .. fit ..'\n",
      " 'new zealand defeated india wickets fourth odi hamilton thursday win first match five match odi series .. india lost international match under rohit sharma captaincy after consecutive victories dating back march .. match witnessed india getting seventh lowest total odi cricket history ..'\n",
      " ...\n",
      " 'according reports new version science fiction film matrix development .. michael jordan reportedly play lead role film .. screenwriter zak penn talks write script film reports added .. actor keanu reeves starred original film followed two sequels ..'\n",
      " 'new music video shows rapper snoop dogg aiming toy gun clown character parodying us president donald trump .. video also shows tv airing news conference headline ronald klump wants deport doggs airing live clown house .. video remixed version song lavender ..'\n",
      " 'madhesi morcha alliance seven political parties withdrawn support pm pushpa kamal dahal led nepal government after failed meet seven day ultimatum fulfil demands including endorsement revised constitution amendment bill .. morcha seats parliament but despite withdrawal support no immediate threat government ..']\n",
      "6.280515193939209  seconds\n",
      "['upgrad learner switches to career in ml al with salary hike'\n",
      " 'delhi techie wins free food from swiggy for one year on cred'\n",
      " 'new zealand end rohit sharma led india match winning streak' ...\n",
      " 'the matrix film to get reboot reports'\n",
      " 'snoop dogg aims gun at clown dressed as trump in new video'\n",
      " 'madhesi morcha withdraws support to nepalese government']\n"
     ]
    }
   ],
   "source": [
    "clean_text = preprocess_data(data2['Text'])  # 클라우드 기준으로 3~4분 정도 소요 됩니다\n",
    "print(clean_text)\n",
    "\n",
    "clean_summary = preprocess_data(data2['Summary'], remove_stopwords=False) # 클라우드 기준 1분정도 소요됩니다.\n",
    "print(clean_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "loving-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98401\n"
     ]
    }
   ],
   "source": [
    "#DataFrame으로 저장합니다\n",
    "data2['Text'] = clean_text\n",
    "data2['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data2.replace('', np.nan, inplace=True)\n",
    "\n",
    "# 결측치 확인 74개의 빈 summary 확인되었습니다\n",
    "data2.isnull().sum()\n",
    "\n",
    "# 결측치 제거\n",
    "data2.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dying-cooperation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 1 이상,34 이하인 샘플의 비율:0.10599485777583562\n"
     ]
    }
   ],
   "source": [
    "bandpass_len(1, 34, data2['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "third-experience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 35 이상,42 이하인 샘플의 비율:0.7018627859472972\n"
     ]
    }
   ],
   "source": [
    "bandpass_len(35, 42, data2['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "vital-migration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 43 이상,3000 이하인 샘플의 비율:0.1921423562768671\n"
     ]
    }
   ],
   "source": [
    "bandpass_len(43, 3000, data2['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-cathedral",
   "metadata": {},
   "source": [
    "\\[35, 42\\]의 구간만 사용하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "breathing-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 42; text_min_len = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "collective-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2[data2['Text'].apply(lambda x: len(x.split()) <= text_max_len \\\n",
    "            and len(x.split()) >= text_min_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "worthy-underground",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 7 이상,13 이하인 샘플의 비율:0.9795986331518591\n"
     ]
    }
   ],
   "source": [
    "bandpass_len(7, 13, data2['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "certified-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_max_len = 13; summary_min_len = 7;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "swedish-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2[data2['Summary'].apply(lambda x: len(x.split()) <= summary_max_len \\\n",
    "            and len(x.split()) >= summary_min_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-mounting",
   "metadata": {},
   "source": [
    "앞과 동일한 처리를 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "maritime-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46545 48430 32283 ... 64323 64084 34778]\n",
      "테스트 데이터의 수 : 13531\n",
      "훈련 데이터의 개수 : 54124\n",
      "훈련 레이블의 개수 : 54124\n",
      "테스트 데이터의 개수 : 13531\n",
      "테스트 레이블의 개수 : 13531\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "data2['decoder_input'] = data2['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data2['decoder_target'] = data2['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data2.head()\n",
    "\n",
    "# np.array로 저장\n",
    "encoder_input = np.array(data2['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data2['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data2['decoder_target']) # 디코더의 레이블\n",
    "\n",
    "# train test split\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-pepper",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "nominated-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer(filters='\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n') # 토크나이저 정의, 마침표와 느낌표, 물음표는 제거하지 않습니다.\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "israeli-western",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 57864\n",
      "등장 빈도가 7번 이하인 희귀 단어의 수: 40833\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 17031\n",
      "단어 집합에서 희귀 단어의 비율: 70.56719203649938\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 4.279249729848357\n"
     ]
    }
   ],
   "source": [
    "threshold = 8  # 희귀 단어 비율을 70%로 맞춰봅니다\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "narrative-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 17046\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab, filters='\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n') # 단어 집합의 크기를 17046으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "neural-premium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17, 1750, 930, 255, 226, 392, 329, 2136, 5974, 11023, 3111, 695, 324, 76, 141, 952, 139, 11023, 268, 410, 259, 1854, 652, 1236, 410, 592, 1101, 1236, 2092, 476, 5311, 255, 3806], [4, 14, 37, 264, 9688, 5062, 1066, 30, 80, 63, 48, 2893, 6691, 8767, 750, 42, 28, 242, 340, 3767, 750, 5797, 3261, 2035, 838, 28, 1471, 266, 1153, 24, 24, 9, 84, 1], [3000, 2126, 99, 6141, 17029, 274, 226, 8171, 214, 129, 32, 69, 2126, 383, 111, 103, 1772, 241, 84, 3, 69, 32, 2126, 7176, 3390, 69, 5975, 2126, 87, 103, 1772]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-caution",
   "metadata": {},
   "source": [
    "summary에 대해서는 decoder_input으로 토큰화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "otherwise-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer(filters='\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "adjustable-composer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 25760\n",
      "등장 빈도가 13번 이하인 희귀 단어의 수: 20920\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 4840\n",
      "단어 집합에서 희귀 단어의 비율: 81.2111801242236\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 11.541676596864784\n"
     ]
    }
   ],
   "source": [
    "threshold = 14\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "corporate-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 512, 492, 163, 7, 18, 261, 17, 1956], [1, 26, 5091, 2405, 329, 2917, 16, 1114, 3, 5407, 1502], [1, 3607, 1070, 160, 42, 310, 2344, 52, 11, 2345], [1, 3018, 6, 71, 2610, 3019, 19, 37, 443, 2145, 1872], [1, 43, 19, 3, 45, 1957, 21, 13, 178, 1250]]\n",
      "target\n",
      "decoder  [[512, 492, 163, 7, 18, 261, 17, 1956, 2], [26, 5091, 2405, 329, 2917, 16, 1114, 3, 5407, 1502, 2], [3607, 1070, 160, 42, 310, 2344, 52, 11, 2345, 2], [3018, 6, 71, 2610, 3019, 19, 37, 443, 2145, 1872, 2], [43, 19, 3, 45, 1957, 21, 13, 178, 1250, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 7693\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab, filters='\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n') \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-production",
   "metadata": {},
   "source": [
    "sostoken과 eostoken이 각각 1, 2로 인코딩된 것과 변환이 제대로 되었음을 확인했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "offshore-burning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 0\n",
      "삭제할 테스트 데이터의 개수 : 0\n",
      "훈련 데이터의 개수 : 54124\n",
      "훈련 레이블의 개수 : 54124\n",
      "테스트 데이터의 개수 : 13531\n",
      "테스트 레이블의 개수 : 13531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# 결측치 제거 - eostoken만 남은 경우를 제거한다. 아마 없을 것이다\n",
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-discussion",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-camping",
   "metadata": {},
   "source": [
    "encoder의 경우, padding을 뒤에 넣으면 context vector가 희석되므로 앞에 padding을 넣습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "arbitrary-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='pre')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='pre')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "equal-influence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "varying-winner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "confirmed-confidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 42)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 42, 128)      2181888     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 42, 256), (N 394240      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 42, 256), (N 525312      lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 128)    984704      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 42, 256), (N 525312      lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, None, 256),  394240      embedding_3[0][0]                \n",
      "                                                                 lstm_6[0][1]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 7693)   1977101     lstm_7[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,982,797\n",
      "Trainable params: 6,982,797\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model2 = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "understanding-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "moved-universe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 42)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 42, 128)      2181888     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 42, 256), (N 394240      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 42, 256), (N 525312      lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 128)    984704      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 42, 256), (N 525312      lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, None, 256),  394240      embedding_3[0][0]                \n",
      "                                                                 lstm_6[0][1]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_6[0][0]                     \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_7[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 7693)   3946509     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 9,083,533\n",
      "Trainable params: 9,083,533\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model2 = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "induced-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "212/212 [==============================] - 149s 652ms/step - loss: 5.5928 - val_loss: 4.8509\n",
      "Epoch 2/50\n",
      "212/212 [==============================] - 141s 666ms/step - loss: 4.8701 - val_loss: 4.6188\n",
      "Epoch 3/50\n",
      "212/212 [==============================] - 142s 669ms/step - loss: 4.6116 - val_loss: 4.3716\n",
      "Epoch 4/50\n",
      "212/212 [==============================] - 142s 670ms/step - loss: 4.3403 - val_loss: 4.1899\n",
      "Epoch 5/50\n",
      "212/212 [==============================] - 141s 666ms/step - loss: 4.1196 - val_loss: 4.0333\n",
      "Epoch 6/50\n",
      "212/212 [==============================] - 141s 667ms/step - loss: 3.9404 - val_loss: 3.9421\n",
      "Epoch 7/50\n",
      "212/212 [==============================] - 143s 673ms/step - loss: 3.7970 - val_loss: 3.8356\n",
      "Epoch 8/50\n",
      "212/212 [==============================] - 142s 670ms/step - loss: 3.6571 - val_loss: 3.7621\n",
      "Epoch 9/50\n",
      "212/212 [==============================] - 143s 673ms/step - loss: 3.5269 - val_loss: 3.6914\n",
      "Epoch 10/50\n",
      "212/212 [==============================] - 142s 668ms/step - loss: 3.4243 - val_loss: 3.6351\n",
      "Epoch 11/50\n",
      "212/212 [==============================] - 143s 672ms/step - loss: 3.3237 - val_loss: 3.5966\n",
      "Epoch 12/50\n",
      "212/212 [==============================] - 143s 675ms/step - loss: 3.2389 - val_loss: 3.5656\n",
      "Epoch 13/50\n",
      "212/212 [==============================] - 143s 674ms/step - loss: 3.1541 - val_loss: 3.5334\n",
      "Epoch 14/50\n",
      "212/212 [==============================] - 143s 674ms/step - loss: 3.0843 - val_loss: 3.5062\n",
      "Epoch 15/50\n",
      "212/212 [==============================] - 143s 674ms/step - loss: 3.0117 - val_loss: 3.4918\n",
      "Epoch 16/50\n",
      "212/212 [==============================] - 143s 675ms/step - loss: 2.9433 - val_loss: 3.4713\n",
      "Epoch 17/50\n",
      "212/212 [==============================] - 144s 677ms/step - loss: 2.8912 - val_loss: 3.4543\n",
      "Epoch 18/50\n",
      "212/212 [==============================] - 144s 679ms/step - loss: 2.8255 - val_loss: 3.4458\n",
      "Epoch 19/50\n",
      "212/212 [==============================] - 142s 671ms/step - loss: 2.7768 - val_loss: 3.4320\n",
      "Epoch 20/50\n",
      "212/212 [==============================] - 141s 667ms/step - loss: 2.7272 - val_loss: 3.4256\n",
      "Epoch 21/50\n",
      "212/212 [==============================] - 142s 668ms/step - loss: 2.6868 - val_loss: 3.4173\n",
      "Epoch 22/50\n",
      "212/212 [==============================] - 144s 680ms/step - loss: 2.6349 - val_loss: 3.4149\n",
      "Epoch 23/50\n",
      "212/212 [==============================] - 142s 672ms/step - loss: 2.5925 - val_loss: 3.4189\n",
      "Epoch 24/50\n",
      "212/212 [==============================] - 143s 676ms/step - loss: 2.5519 - val_loss: 3.4093\n",
      "Epoch 25/50\n",
      "212/212 [==============================] - 142s 672ms/step - loss: 2.5032 - val_loss: 3.4067\n",
      "Epoch 26/50\n",
      "212/212 [==============================] - 141s 664ms/step - loss: 2.4727 - val_loss: 3.4076\n",
      "Epoch 27/50\n",
      "212/212 [==============================] - 142s 669ms/step - loss: 2.4381 - val_loss: 3.4167\n",
      "Epoch 00027: early stopping\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model2.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "engaged-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "rapid-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "biological-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cardiovascular-settlement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 12160\n",
      "원문 : apology email sent riders new york ride hailing startup uber said expanding quickly failed prioritise people helped get us admitting fallen short uber explained radical change following investigation startup questionable work environment \n",
      "실제 요약 : we failed to prioritise riders by so uber \n",
      "예측 요약 :  uber says he is not scared of uber employee\n",
      "\n",
      "\n",
      "id: 12161\n",
      "원문 : irrfan khan saba qamar starrer hindi medium earned crore three days release china april film earned opening day aamir khan dangal salman khan bajrangi bhaijaan released china directed chaudhary hindi medium released india may \n",
      "실제 요약 : hindi medium earns crore in days of its china release \n",
      "예측 요약 :  irrfan khan starrer tubelight hits the theatres\n",
      "\n",
      "\n",
      "id: 12162\n",
      "원문 : passenger charged interfering cabin crew after new york bound jetblue flight forced make emergency landing due drunken behaviour fbi accused robin touching man head manner attacking flight attendant further put plastic but managed break \n",
      "실제 요약 : woman flyer charged for touching man head \n",
      "예측 요약 :  united airlines diverted after passenger passenger dragged off\n",
      "\n",
      "\n",
      "id: 12163\n",
      "원문 : mit researchers built sensor equipped bacteria stomach bleeding sensor successfully tested pigs converts bacterial response wireless signal read smartphone researchers estimate battery could power inch long device months continuous use \n",
      "실제 요약 : mit designs sensor to detect in stomach \n",
      "예측 요약 :  mit develops printed device that can make objects\n",
      "\n",
      "\n",
      "id: 12164\n",
      "원문 : singer sona mohapatra criticised sonu nigam backing anu malik amid sexual harassment allegations questioning proof sona tweeted much sympathy millionaire losing work much empathy privileged family tortured wrote another tweet multiple women men malik abusive behaviour \n",
      "실제 요약 : sympathy for millionaire sona to nigam on backing anu over metoo \n",
      "예측 요약 :  sona slams brad over sexual harassment allegations\n",
      "\n",
      "\n",
      "id: 12165\n",
      "원문 : akshay kumar shared video sister alka bhatia said biggest gift received lesson apna dhyan added realised lesson importance daughter went abroad higher studies video ends message not protect sister empower \n",
      "실제 요약 : was the best gift akshay sister \n",
      "예측 요약 :  my wife is not even for me akshay on sridevi death\n",
      "\n",
      "\n",
      "id: 12166\n",
      "원문 : attempt increase numbers sea turtles maharashtra forest department announced reward persons share information sea turtle eggs officials said sea turtles come breeding maharashtra since last eight ten years no sea turtle eggs found area \n",
      "실제 요약 : forest dept announces reward for info on sea turtle eggs \n",
      "예측 요약 :  ghaziabad civic body plans to mine\n",
      "\n",
      "\n",
      "id: 12167\n",
      "원문 : veteran fighter known sheikh killing least islamic state terrorists killed iraq battle retake town militant group abu al also fought late iraqi dictator saddam hussein war iran took part conflicts dating back arab israeli war \n",
      "실제 요약 : anti isis of killed in iraq \n",
      "예측 요약 :  isis militants killed in syria\n",
      "\n",
      "\n",
      "id: 12168\n",
      "원문 : promotional offers telecom firms like reliance jio give free voice data not responsible industry falling financial health according telecom regulator trai trai believes telecom commission contention free promotional offers allowed responsible fall lower licence fee payments government incorrect \n",
      "실제 요약 : jio free offer has not caused fall in industry health trai \n",
      "예측 요약 :  jio jio to reduce new customers from jio\n",
      "\n",
      "\n",
      "id: 12169\n",
      "원문 : amazon thursday announced plan lower whole foods prices offer amazon prime members discount grocery chain products news slashed billion market value five grocery store chains total market five chains include walmart fell billion thursday \n",
      "실제 요약 : rivals lose bn as amazon plans to cut whole prices \n",
      "예측 요약 :  amazon amazon amazon opens its own tax data\n",
      "\n",
      "\n",
      "id: 12170\n",
      "원문 : bollywood actor ranbir kapoor received fc barcelona jersey signed lionel messi th birthday bollywood actor self confessed fan club called messi best player earth posed jersey barcelona number jersey also actor initials rk written back \n",
      "실제 요약 : ranbir receives barca jersey signed by messi on th day \n",
      "예측 요약 :  ranbir kapoor taimur taimur jab taimur wedding taimur jab taimur\n",
      "\n",
      "\n",
      "id: 12171\n",
      "원문 : rajya sabha wednesday passed finance bill along five amendments proposed opposition bill require approval lok sabha choose reject amendments following would still considered passed parliament bill makes aadhaar mandatory filing income tax returns \n",
      "실제 요약 : finance bill passed in rajya sabha with five \n",
      "예측 요약 :  bill to bill bill passed bill passed bill passed in parliament\n",
      "\n",
      "\n",
      "id: 12172\n",
      "원문 : bharatiya janata party defeated ruling congress assembly elections form government himachal pradesh secured required majority seats member assembly whereas congress seats far bjp earlier declared former state cm prem kumar chief ministerial candidate \n",
      "실제 요약 : bjp defeats congress to form government in himachal pradesh \n",
      "예측 요약 :  congress jd wins seats in himachal assembly polls\n",
      "\n",
      "\n",
      "id: 12173\n",
      "원문 : currency public reached record high trillion double low trillion post demonetisation rbi data showed total currency circulation also doubled trillion post demonetisation low notably currency public stood trillion before demonetisation \n",
      "실제 요약 : cash with public doubles since note ban to record tn \n",
      "예측 요약 :  rupee falls to trillion in world economy\n",
      "\n",
      "\n",
      "id: 12174\n",
      "원문 : least eight people reportedly killed sunday attack security headquarters yemeni government city attack involved car bombing explosive belt suicide bomber while officials suspect attack carried al qaeda militants islamic state claimed responsibility attack \n",
      "실제 요약 : killed in attack on govt security headquarters \n",
      "예측 요약 :  killed in suicide attack in egypt\n",
      "\n",
      "\n",
      "id: 12175\n",
      "원문 : actress anushka sharma took instagram share picture husband cricketer virat kohli honeymoon heaven literally wrote alongside picture within two hours photo received million likes instagram anushka married indian cricket team captain italy intimate wedding ceremony \n",
      "실제 요약 : anushka shares pic with husband virat from their honeymoon \n",
      "예측 요약 :  anushka shares pic with virat anushka sharma with her\n",
      "\n",
      "\n",
      "id: 12176\n",
      "원문 : delhi based startup raised crore series round funding valuation around crore investment came cash deal existing investor info edge india parent online job portal shareholding increasing info edge picked around stake november \n",
      "실제 요약 : delhi startup raises cr at cr valuation \n",
      "예측 요약 :  raises crore from softbank in paytm\n",
      "\n",
      "\n",
      "id: 12177\n",
      "원문 : former mumbai mayor padma shri awardee nana died age sunday mumbai awarded padma shri recognition work social sector two message board marine drive reflection personality maharashtra cm devendra fadnavis said \n",
      "실제 요약 : former mumbai mayor and padma shri nana dies at \n",
      "예측 요약 :  padma shri passes away aged\n",
      "\n",
      "\n",
      "id: 12178\n",
      "원문 : russian president vladimir putin suggested us president donald trump crisis ukraine could solved referendum according bloomberg russia annexed crimea pro russian separatists declared independence ukraine region following not internationally recognised us said not recognise russia annexation crimea \n",
      "실제 요약 : putin discussed referendum on ukraine with trump report \n",
      "예측 요약 :  putin putin putin putin on putin\n",
      "\n",
      "\n",
      "id: 12179\n",
      "원문 : japan biggest automaker toyota saturday said sold shares elon musk led tesla cancelled tie jointly develop electric vehicles toyota said shares sold end part regular review investments company bought approximately stake tesla million \n",
      "실제 요약 : toyota sells all shares in tesla scraps tie up to make cars \n",
      "예측 요약 :  tesla car car to sell musk elon musk car car\n",
      "\n",
      "\n",
      "id: 12180\n",
      "원문 : tesla model electric car reportedly caught fire california after towed garage due flat car made sound produced smoke before surrounded flames owner said fire department managed control fire later likely due heat build batteries \n",
      "실제 요약 : tesla car catches fire in us few hours later \n",
      "예측 요약 :  tesla car catches fire catches fire in car crash\n",
      "\n",
      "\n",
      "id: 12181\n",
      "원문 : after kerala cm pinarayi vijayan assured support man protesting days custodial death brother one four accused cops demanded cbi inquiry case police official added four accused willing give lakh deceased family found guilty \n",
      "실제 요약 : kerala cop accused in death demands cbi probe \n",
      "예측 요약 :  kerala cm visits kerala cm over child death\n",
      "\n",
      "\n",
      "id: 12182\n",
      "원문 : australia plans strengthen anti money laundering laws regulate digital currencies bitcoins crackdown terror funding reports said threat serious financial crime constantly evolving new technologies emerge australian justice minister michael said however minister not specify legislation would introduced \n",
      "실제 요약 : australia plans to regulate bitcoin to counter terror funds \n",
      "예측 요약 :  australia offers to pay crore to media deal\n",
      "\n",
      "\n",
      "id: 12183\n",
      "원문 : latest icc rankings bowlers first nine positions held spinners six first seven positions occupied leg spinners further indian fast bowler jasprit bumrah only pacer top occupying th position third placed yuzvendra chahal only indian top \n",
      "실제 요약 : all top bowlers spinners bumrah only pacer in top \n",
      "예측 요약 :  rahane once bowled for first time in years\n",
      "\n",
      "\n",
      "id: 12184\n",
      "원문 : report estimates crore adults around world risk serious disease due lack physical exercise stated high income countries uk us among least active report further said women tend men almost every region throughout world \n",
      "실제 요약 : lack of exercise puts crore at risk of disease who \n",
      "예측 요약 :  lakh note ban on note ban in note ban\n",
      "\n",
      "\n",
      "id: 12185\n",
      "원문 : official pictures prince harry meghan markle royal wedding released images show one group picture sides family another group picture children black white picture prince harry meghan royal couple addressed duke duchess sussex \n",
      "실제 요약 : official pics of prince harry and meghan royal wedding out \n",
      "예측 요약 :  meghan markle to marry prince harry meghan wedding\n",
      "\n",
      "\n",
      "id: 12186\n",
      "원문 : us based co working space startup wework tuesday said raised billion funding japan softbank latest funding form warrant under softbank pay wework billion january remaining april deal would raise wework valuation least billion according startup \n",
      "실제 요약 : co working space startup wework raises bn from softbank \n",
      "예측 요약 :  tiger becomes first startup raises billion\n",
      "\n",
      "\n",
      "id: 12187\n",
      "원문 : woman block bihar muzaffarpur district filed police complaint father law brother law lack toilet home police called two accused police station woman reportedly made father law sign bond would get toilet constructed soon \n",
      "실제 요약 : woman files fir against father in law over no toilet at home \n",
      "예측 요약 :  woman booked for molestation by husband in bihar\n",
      "\n",
      "\n",
      "id: 12188\n",
      "원문 : tickets india football match kenya today mumbai sold two days after indian skipper sunil chhetri posted video urging people show matches stadiums stadium capacity around also hosted india last match only spectators following chhetri posted video \n",
      "실제 요약 : indian football team match sold out after chhetri plea \n",
      "예측 요약 :  indian football team team out of ground to draw in ipl\n",
      "\n",
      "\n",
      "id: 12189\n",
      "원문 : telecom minister manoj sinha said government ensure no spectrum shortage india not lag behind rest world launching services operational six months telecom secretary aruna said government september laid roadmap rollout networks india \n",
      "실제 요약 : india ready to launch at with global govt \n",
      "예측 요약 :  govt to be allowed to make jobs in india says chief\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(12160, 12190):\n",
    "    print('id:', i)\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-independence",
   "metadata": {},
   "source": [
    "아직 마침표 처리가 안 된 것을 알 수 있습니다. 이는 tokenizer의 filters에 놓친 부분이 있어서입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-oriental",
   "metadata": {},
   "source": [
    "잘 된 요약:\n",
    "\n",
    "id: 12174\n",
    "\n",
    "원문 : least eight people reportedly killed sunday attack security headquarters yemeni government city attack involved car bombing explosive belt suicide bomber while officials suspect attack carried al qaeda militants islamic state claimed responsibility attack \n",
    "\n",
    "실제 요약 : killed in attack on govt security headquarters \n",
    "\n",
    "예측 요약 :  killed in suicide attack in egypt\n",
    "\n",
    "id: 12175\n",
    "\n",
    "원문 : actress anushka sharma took instagram share picture husband cricketer virat kohli honeymoon heaven literally wrote alongside picture within two hours photo received million likes instagram anushka married indian cricket team captain italy intimate wedding ceremony \n",
    "\n",
    "실제 요약 : anushka shares pic with husband virat from their honeymoon \n",
    "\n",
    "예측 요약 :  anushka shares pic with virat anushka sharma with her\n",
    "\n",
    "id: 12177\n",
    "\n",
    "원문 : former mumbai mayor padma shri awardee nana died age sunday mumbai awarded padma shri recognition work social sector two message board marine drive reflection personality maharashtra cm devendra fadnavis said \n",
    "\n",
    "실제 요약 : former mumbai mayor and padma shri nana dies at \n",
    "\n",
    "예측 요약 :  padma shri passes away aged\n",
    "\n",
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "      \n",
    "      \n",
    "동어 반복:\n",
    "\n",
    "\n",
    "id: 12180\n",
    "\n",
    "원문 : tesla model electric car reportedly caught fire california after towed garage due flat car made sound produced smoke before surrounded flames owner said fire department managed control fire later likely due heat build batteries \n",
    "\n",
    "실제 요약 : tesla car catches fire in us few hours later \n",
    "\n",
    "예측 요약 :  tesla car catches fire catches fire in car crash\n",
    "\n",
    "id: 12184\n",
    "\n",
    "원문 : report estimates crore adults around world risk serious disease due lack physical exercise stated high income countries uk us among least active report further said women tend men almost every region throughout world \n",
    "\n",
    "실제 요약 : lack of exercise puts crore at risk of disease who \n",
    "\n",
    "예측 요약 :  lakh note ban on note ban in note ban\n",
    "\n",
    "id: 12171\n",
    "\n",
    "원문 : rajya sabha wednesday passed finance bill along five amendments proposed opposition bill require approval lok sabha choose reject amendments following would still considered passed parliament bill makes aadhaar mandatory filing income tax returns \n",
    "\n",
    "실제 요약 : finance bill passed in rajya sabha with five \n",
    "\n",
    "예측 요약 :  bill to bill bill passed bill passed bill passed in parliament"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-attitude",
   "metadata": {},
   "source": [
    "# Trial 3\n",
    "\n",
    "온점이 온전히 인식되도록 수정하고 vocab size도 수정하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-retreat",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "distant-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "reported-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data3[['text', 'headlines']]\n",
    "data3.rename(columns = {'text': 'Text', 'headlines': 'Summary'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-richards",
   "metadata": {},
   "source": [
    "## preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "reduced-knife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.682997465133667  seconds\n",
      "['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience .. program upgrad degree career support helped transition data scientist tech mahindra salary hike .. upgrad online power learning powered lakh careers ..'\n",
      " 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year .. pranav kaushik delhi techie bagged reward after spending cred coins .. users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult .. fit ..'\n",
      " 'new zealand defeated india wickets fourth odi hamilton thursday win first match five match odi series .. india lost international match under rohit sharma captaincy after consecutive victories dating back march .. match witnessed india getting seventh lowest total odi cricket history ..'\n",
      " ...\n",
      " 'according reports new version science fiction film matrix development .. michael jordan reportedly play lead role film .. screenwriter zak penn talks write script film reports added .. actor keanu reeves starred original film followed two sequels ..'\n",
      " 'new music video shows rapper snoop dogg aiming toy gun clown character parodying us president donald trump .. video also shows tv airing news conference headline ronald klump wants deport doggs airing live clown house .. video remixed version song lavender ..'\n",
      " 'madhesi morcha alliance seven political parties withdrawn support pm pushpa kamal dahal led nepal government after failed meet seven day ultimatum fulfil demands including endorsement revised constitution amendment bill .. morcha seats parliament but despite withdrawal support no immediate threat government ..']\n",
      "6.7240149974823  seconds\n",
      "['upgrad learner switches to career in ml al with salary hike'\n",
      " 'delhi techie wins free food from swiggy for one year on cred'\n",
      " 'new zealand end rohit sharma led india match winning streak' ...\n",
      " 'the matrix film to get reboot reports'\n",
      " 'snoop dogg aims gun at clown dressed as trump in new video'\n",
      " 'madhesi morcha withdraws support to nepalese government']\n"
     ]
    }
   ],
   "source": [
    "clean_text = preprocess_data(data3['Text'])  # 클라우드 기준으로 3~4분 정도 소요 됩니다\n",
    "print(clean_text)\n",
    "\n",
    "clean_summary = preprocess_data(data3['Summary'], remove_stopwords=False) # 클라우드 기준 1분정도 소요됩니다.\n",
    "print(clean_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "presidential-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98401\n"
     ]
    }
   ],
   "source": [
    "#DataFrame으로 저장합니다\n",
    "data3['Text'] = clean_text\n",
    "data3['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data3.replace('', np.nan, inplace=True)\n",
    "\n",
    "# 결측치 확인 74개의 빈 summary 확인되었습니다\n",
    "data3.isnull().sum()\n",
    "\n",
    "# 결측치 제거\n",
    "data3.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-weekend",
   "metadata": {},
   "source": [
    "\\[37, 40\\]의 구간만 사용하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "fifty-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 40; text_min_len = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "different-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data3[data3['Text'].apply(lambda x: len(x.split()) <= text_max_len \\\n",
    "            and len(x.split()) >= text_min_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "eligible-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_max_len = 14; summary_min_len = 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "smoking-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data3[data3['Summary'].apply(lambda x: len(x.split()) <= summary_max_len \\\n",
    "            and len(x.split()) >= summary_min_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "flexible-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11690 17440 21548 ... 32998 12988 24963]\n",
      "테스트 데이터의 수 : 7829\n",
      "훈련 데이터의 개수 : 31318\n",
      "훈련 레이블의 개수 : 31318\n",
      "테스트 데이터의 개수 : 7829\n",
      "테스트 레이블의 개수 : 7829\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "data3['decoder_input'] = data3['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data3['decoder_target'] = data3['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data3.head()\n",
    "\n",
    "# np.array로 저장\n",
    "encoder_input = np.array(data3['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data3['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data3['decoder_target']) # 디코더의 레이블\n",
    "\n",
    "# train test split\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-anaheim",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "equivalent-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer(filters='\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n') # 토크나이저 정의, 마침표와 느낌표, 물음표는 제거하지 않습니다.\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "twenty-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 45832\n",
      "등장 빈도가 10번 이하인 희귀 단어의 수: 35395\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 10437\n",
      "단어 집합에서 희귀 단어의 비율: 77.22770116948857\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 7.753856142251195\n"
     ]
    }
   ],
   "source": [
    "threshold = 11  # 희귀 단어 비율을 66%로 맞춰봅니다\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "scientific-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 10437\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab, filters='\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n') # 단어 집합의 크기를 17046으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "actual-anatomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[256, 86, 4167, 3302, 60, 4487, 2079, 89, 42, 1443, 175, 23, 212, 3149, 154, 3, 2748, 1515, 779, 7938, 3027, 1, 89, 683, 3302, 1475, 1986, 23, 125, 1903, 5285, 220, 3, 3027, 1709, 1], [5, 15, 475, 417, 45, 3116, 174, 493, 803, 920, 6713, 2749, 56, 1, 1928, 1226, 942, 8246, 5, 15, 42, 2704, 79, 7663, 12, 1, 34, 123, 437, 527, 1568, 447, 42, 4400, 696, 5417, 1], [299, 152, 2907, 1750, 1987, 78, 1777, 3, 1341, 666, 938, 7939, 1, 718, 1410, 1750, 262, 1, 78, 1, 78, 1552, 1008, 4168, 3071, 532, 1, 78, 1, 72, 3, 1, 666, 40, 1135, 1057, 1777, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-logan",
   "metadata": {},
   "source": [
    "summary에 대해서는 decoder_input으로 토큰화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "unsigned-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer(filters='\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "focal-satisfaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 20877\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 11019\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 9858\n",
      "단어 집합에서 희귀 단어의 비율: 52.78057192125305\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 4.323489435314469\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "received-forum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 113, 32, 3673, 667, 132, 4, 25, 205, 92, 54], [1, 19, 7021, 607, 3050, 816, 877, 716, 3, 1461, 83], [1, 143, 78, 3674, 1121, 4571, 34, 4], [1, 1462, 779, 1807, 404, 3, 50, 7022, 14, 6167], [1, 77, 268, 2187, 1393, 1265, 3, 717, 12, 470, 217]]\n",
      "target\n",
      "decoder  [[113, 32, 3673, 667, 132, 4, 25, 205, 92, 54, 2], [19, 7021, 607, 3050, 816, 877, 716, 3, 1461, 83, 2], [143, 78, 3674, 1121, 4571, 34, 4, 2], [1462, 779, 1807, 404, 3, 50, 7022, 14, 6167, 2], [77, 268, 2187, 1393, 1265, 3, 717, 12, 470, 217, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 11019\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab, filters='\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n') \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-surname",
   "metadata": {},
   "source": [
    "sostoken과 eostoken이 각각 1, 2로 인코딩된 것과 변환이 제대로 되었음을 확인했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "numeric-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 0\n",
      "삭제할 테스트 데이터의 개수 : 0\n",
      "훈련 데이터의 개수 : 31318\n",
      "훈련 레이블의 개수 : 31318\n",
      "테스트 데이터의 개수 : 7829\n",
      "테스트 레이블의 개수 : 7829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# 결측치 제거 - eostoken만 남은 경우를 제거한다. 아마 없을 것이다\n",
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-knife",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-boards",
   "metadata": {},
   "source": [
    "encoder의 경우, padding을 뒤에 넣으면 context vector가 희석되므로 앞에 padding을 넣습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "weekly-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='pre')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='pre')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "higher-history",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "norwegian-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "bridal-typing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 40, 128)      1335936     input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  [(None, 40, 256), (N 394240      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, 40, 256), (N 525312      lstm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 128)    1410432     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  [(None, 40, 256), (N 525312      lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  [(None, None, 256),  394240      embedding_7[0][0]                \n",
      "                                                                 lstm_14[0][1]                    \n",
      "                                                                 lstm_14[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 11019)  2831883     lstm_15[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,417,355\n",
      "Trainable params: 7,417,355\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model2 = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "reflected-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "chubby-horizon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 40, 128)      1335936     input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  [(None, 40, 256), (N 394240      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, 40, 256), (N 525312      lstm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 128)    1410432     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  [(None, 40, 256), (N 525312      lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  [(None, None, 256),  394240      embedding_7[0][0]                \n",
      "                                                                 lstm_14[0][1]                    \n",
      "                                                                 lstm_14[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_14[0][0]                    \n",
      "                                                                 lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_15[0][0]                    \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, None, 11019)  5652747     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,369,547\n",
      "Trainable params: 10,369,547\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model2 = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "recreational-immunology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "123/123 [==============================] - 94s 680ms/step - loss: 5.8097 - val_loss: 4.8810\n",
      "Epoch 2/50\n",
      "123/123 [==============================] - 83s 673ms/step - loss: 4.8910 - val_loss: 4.6263\n",
      "Epoch 3/50\n",
      "123/123 [==============================] - 83s 677ms/step - loss: 4.6869 - val_loss: 4.5085\n",
      "Epoch 4/50\n",
      "123/123 [==============================] - 83s 677ms/step - loss: 4.5275 - val_loss: 4.3659\n",
      "Epoch 5/50\n",
      "123/123 [==============================] - 84s 679ms/step - loss: 4.3522 - val_loss: 4.2553\n",
      "Epoch 6/50\n",
      "123/123 [==============================] - 83s 678ms/step - loss: 4.1866 - val_loss: 4.1379\n",
      "Epoch 7/50\n",
      "123/123 [==============================] - 83s 678ms/step - loss: 4.0600 - val_loss: 4.0526\n",
      "Epoch 8/50\n",
      "123/123 [==============================] - 83s 677ms/step - loss: 3.9120 - val_loss: 3.9927\n",
      "Epoch 9/50\n",
      "123/123 [==============================] - 83s 679ms/step - loss: 3.7991 - val_loss: 3.9367\n",
      "Epoch 10/50\n",
      "123/123 [==============================] - 84s 680ms/step - loss: 3.6823 - val_loss: 3.8862\n",
      "Epoch 11/50\n",
      "123/123 [==============================] - 84s 680ms/step - loss: 3.5788 - val_loss: 3.8626\n",
      "Epoch 12/50\n",
      "123/123 [==============================] - 84s 680ms/step - loss: 3.4779 - val_loss: 3.8278\n",
      "Epoch 13/50\n",
      "123/123 [==============================] - 84s 680ms/step - loss: 3.3961 - val_loss: 3.7788\n",
      "Epoch 14/50\n",
      "123/123 [==============================] - 83s 677ms/step - loss: 3.3021 - val_loss: 3.7656\n",
      "Epoch 15/50\n",
      "123/123 [==============================] - 84s 679ms/step - loss: 3.2206 - val_loss: 3.7457\n",
      "Epoch 16/50\n",
      "123/123 [==============================] - 84s 681ms/step - loss: 3.1445 - val_loss: 3.7281\n",
      "Epoch 17/50\n",
      "123/123 [==============================] - 84s 682ms/step - loss: 3.0694 - val_loss: 3.7237\n",
      "Epoch 18/50\n",
      "123/123 [==============================] - 84s 681ms/step - loss: 3.0004 - val_loss: 3.7172\n",
      "Epoch 19/50\n",
      "123/123 [==============================] - 84s 683ms/step - loss: 2.9296 - val_loss: 3.7064\n",
      "Epoch 20/50\n",
      "123/123 [==============================] - 84s 679ms/step - loss: 2.8728 - val_loss: 3.6956\n",
      "Epoch 21/50\n",
      "123/123 [==============================] - 83s 679ms/step - loss: 2.8159 - val_loss: 3.6971\n",
      "Epoch 22/50\n",
      "123/123 [==============================] - 84s 680ms/step - loss: 2.7525 - val_loss: 3.6954\n",
      "Epoch 23/50\n",
      "123/123 [==============================] - 83s 678ms/step - loss: 2.6921 - val_loss: 3.6852\n",
      "Epoch 24/50\n",
      "123/123 [==============================] - 83s 679ms/step - loss: 2.6395 - val_loss: 3.6857\n",
      "Epoch 25/50\n",
      "123/123 [==============================] - 84s 680ms/step - loss: 2.5855 - val_loss: 3.6893\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model2.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "disabled-particle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "stable-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "thermal-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "every-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "secure-coast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6500\n",
      "원문 : human resource development ministry reportedly planning replace western convocation indian ethnic wear .. ministry meeting officials idea viewing colonial past .. notice issued .. attire match conditions .. reflect culture hrd official said .. \n",
      "실제 요약 : govt plans to replace convocation with ethnic \n",
      "예측 요약 :  government to launch new budget\n",
      "\n",
      "\n",
      "id: 6501\n",
      "원문 : kerala government temporarily banned forest areas after nine people killed forest fire while hills tamil nadu .. state forest department also directed take necessary prevent forest fires .. further tour operators directed not undertake activities .. \n",
      "실제 요약 : kerala temporarily bans after tn fire kills \n",
      "예측 요약 :  illegal rainfall found in tn floods\n",
      "\n",
      "\n",
      "id: 6502\n",
      "원문 : according reports mark paid .. million while michelle williams got film money world .. reports added team fee while michelle not told deal .. director scott claimed actors nothing .. \n",
      "실제 요약 : mark was paid crore michelle for report \n",
      "예측 요약 :  it is the of the life of the life of india\n",
      "\n",
      "\n",
      "id: 6503\n",
      "원문 : amazon founder world richest person jeff bezos shared childhood picture twitter said .. lucky exposed tech young age .. hope new amazon future engineer program kids today added amazon program underprivileged children young adults .. \n",
      "실제 요약 : lucky to be exposed to tech so young bezos on childhood pic \n",
      "예측 요약 :  bezos founder shares pic of his iphone on his day\n",
      "\n",
      "\n",
      "id: 6504\n",
      "원문 : actress former bigg boss contestant accused fraud event management company .. reportedly supposed perform wedding reception but not make function costing company loss nearly lakh .. event manager also accused blackmailing .. \n",
      "실제 요약 : bigg boss contestant karishma tanna accused of fraud \n",
      "예측 요약 :  shilpa shilpa to be paid crore on tv show reports\n",
      "\n",
      "\n",
      "id: 6505\n",
      "원문 : television personality kim kardashian while opening robbery incident paris said mentally prepared raped killed robbers .. grabbed legs .. no clothes under said while describing incident .. took experience not let added kim .. \n",
      "실제 요약 : mentally prepared myself to be raped kim on paris robbery \n",
      "예측 요약 :  was dead suicide after being was raped in suicide\n",
      "\n",
      "\n",
      "id: 6506\n",
      "원문 : nasa operation discovered iceberg three times size manhattan antarctica .. named believed measure square km according estimates us national ice centre .. however satellite imagery showed main iceberg may not last long already beginning break nasa said .. \n",
      "실제 요약 : nasa finds antarctic iceberg times the size of \n",
      "예측 요약 :  scientists identify ice ice found in study\n",
      "\n",
      "\n",
      "id: 6507\n",
      "원문 : actor salman khan launching hussain lead actress opposite brother law aayush sharma upcoming production loveratri .. film also mark acting debut aayush .. earlier salman tweeted gayi resulting reports said get married soon .. \n",
      "실제 요약 : salman to launch opposite his brother in law aayush \n",
      "예측 요약 :  salman to play salman in salman khan film report\n",
      "\n",
      "\n",
      "id: 6508\n",
      "원문 : centre wednesday criticised former uttar pradesh chief minister akhilesh yadav saying martyrs several indian states but none prime minister narendra modi home state gujarat .. union minister state pandey termed akhilesh yadav statement also demanded apology .. \n",
      "실제 요약 : govt criticises akhilesh yadav for his statement on martyrs \n",
      "예측 요약 :  no one of the pm modi is the pm modi on dec govt\n",
      "\n",
      "\n",
      "id: 6509\n",
      "원문 : former pakistan high commissioner abdul thanked india bid country after serving envoy delhi four years .. appointed high commissioner india tweeted goodbye india thanks everything .. succeeded sohail mahmood expected take responsibilities around mid august .. \n",
      "실제 요약 : goodbye india and thanks for everything former pak envoy \n",
      "예측 요약 :  indian army chief appointed as india india\n",
      "\n",
      "\n",
      "id: 6510\n",
      "원문 : after congress criticised video surgical strikes union law minister ravi shankar prasad thursday said congress party not mainstream one .. congress party repeatedly giving not mainstream political party .. breaking morale country minister added .. \n",
      "실제 요약 : congress and not mainstream party bjp \n",
      "예측 요약 :  congress is not defeat congress on congress leader\n",
      "\n",
      "\n",
      "id: 6511\n",
      "원문 : encrypted messaging app said popular among us president donald trump staff sued falsely claiming people cannot take screenshots messages app .. lawsuit notes app block screenshots ios but security feature fails user message windows app .. \n",
      "실제 요약 : chat app sued on false claims of protection from \n",
      "예측 요약 :  twitter trump account to sue trump account\n",
      "\n",
      "\n",
      "id: 6512\n",
      "원문 : national payments corporation india backed app updated features like bills collecting payments .. app allows users send receive money users mobile number payment address available android app store .. launched app version upi .. \n",
      "실제 요약 : bhim app allows users to split bills and collect payments \n",
      "예측 요약 :  google launches app to launch users\n",
      "\n",
      "\n",
      "id: 6513\n",
      "원문 : manto director das saturday tweeted disappointed manto not seen theatres pakistan .. keen belongs countries equally .. film based life pakistani author born british india hasan manto .. film stars nawazuddin siddiqui rishi kapoor divya dutta .. \n",
      "실제 요약 : disappointed that manto will not be shown in pakistan nandita \n",
      "예측 요약 :  baahubali is the most popular indian film on india film\n",
      "\n",
      "\n",
      "id: 6514\n",
      "원문 : shillong host second festival november .. known india international festival hosted meghalaya government feature bicycle rallies archery beauty pageant .. also live music dance performances stalls region .. \n",
      "실제 요약 : shillong to host cherry festival \n",
      "예측 요약 :  flag discovered in india\n",
      "\n",
      "\n",
      "id: 6515\n",
      "원문 : twenty four year old peter mumbai known india first one man band play musical time .. play total including piano .. peter shared first one man band video may .. \n",
      "실제 요약 : india st one man band plays at one time \n",
      "예측 요약 :  yr old dances to be named in the world day\n",
      "\n",
      "\n",
      "id: 6516\n",
      "원문 : commenting relationship americans russians model actress anderson said americans blame russia anything goes wrong .. adding russians emotional engaged wanting right thing anderson said people think russians .. .. further said america .. \n",
      "실제 요약 : americans are to blame russia pamela anderson \n",
      "예측 요약 :  twitter slams news for calling people to create\n",
      "\n",
      "\n",
      "id: 6517\n",
      "원문 : gurudwara wife punjab arrested allegedly using torn pages holy book sons .. matter reported student union found torn pages near boys school .. union leaders said found torn pages visited residence .. \n",
      "실제 요약 : couple held for using pages of holy book to wrap kids lunch \n",
      "예측 요약 :  man arrested for molesting woman to help\n",
      "\n",
      "\n",
      "id: 6518\n",
      "원문 : elon musk led tesla asked suppliers return portion payments order turn profitable according wall street journal .. claimed startup requesting back portion suppliers payments since .. tesla claimed requests standard part procurement negotiations improve competitive advantage report added .. \n",
      "실제 요약 : tesla asked for refunds to turn profitable report \n",
      "예측 요약 :  musk tesla backs tesla for tesla workers\n",
      "\n",
      "\n",
      "id: 6519\n",
      "원문 : israeli border police arrested man deliberately got arrested six phones prisoners .. man described israeli police arab trying scale security fence surrounding jerusalem placed custody .. however body search done minutes before putting inmates revealed carrying phones .. \n",
      "실제 요약 : man gets arrested to give phones hid in body to inmates \n",
      "예측 요약 :  man arrested for molesting man who arrested for molesting man\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6500, 6520):\n",
    "    print('id:', i)\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-madonna",
   "metadata": {},
   "source": [
    "직전의 단계에 비해 오히려 요악의 품질이 떨어진 것을 알 수 있습니다. 문장부호는 너무 자주 나와 (등장빈도 1위) 학습에 혼란을 주는 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-rolling",
   "metadata": {},
   "source": [
    "# 추출적 요약 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "statewide-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "parliamentary-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = data4[['text', 'headlines']]\n",
    "data4.rename(columns = {'text': 'Text', 'headlines': 'Summary'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "comic-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "hindu-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(data4['Text'].apply(str).iloc[1], ratio=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "confidential-leave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.\""
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4['Text'].apply(str).iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "loaded-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_news(idx):\n",
    "    for i in idx:\n",
    "        print('원문\\n',data4['Text'].apply(str).iloc[i], '\\n')\n",
    "        print('요약\\n',summarize(data4['Text'].apply(str).iloc[i], ratio=0.4),'\\n\\n')\n",
    "        print('======================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "developmental-algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문\n",
      " Italian third division football side Lucchese's head coach Giancarlo Favarin has been banned for five months for headbutting Alessandria's assistant coach Gaetano Mancino during a brawl following the teams' 2-2 draw on Sunday. Mancino was caught off-balance and knocked to the ground after the headbutt. Earlier in the match, Favarin had told his own player to break an opponent's legs. \n",
      "\n",
      "요약\n",
      " Italian third division football side Lucchese's head coach Giancarlo Favarin has been banned for five months for headbutting Alessandria's assistant coach Gaetano Mancino during a brawl following the teams' 2-2 draw on Sunday. \n",
      "\n",
      "\n",
      "======================================================================================\n",
      "원문\n",
      " The death toll from the mine dam collapse in Brazil last week has risen to 58 and more than 300 others are still missing, authorities said. The dam break caused a sea of muddy sludge to bury the cafeteria at the iron-ore mine where workers were eating lunch, before engulfing nearby houses, vehicles and roads. \n",
      "\n",
      "요약\n",
      "  \n",
      "\n",
      "\n",
      "======================================================================================\n",
      "원문\n",
      " Road Transport Minister Nitin Gadkari on Saturday said the leadership should have the tendency to own up to defeat and failures, adding that loyalty towards the organisation is not proved until one takes responsibility for defeat. \"Success has many fathers but failure is an orphan,\" he added. Gadkari's remarks come days after BJP lost Assembly elections in three states. \n",
      "\n",
      "요약\n",
      " Road Transport Minister Nitin Gadkari on Saturday said the leadership should have the tendency to own up to defeat and failures, adding that loyalty towards the organisation is not proved until one takes responsibility for defeat. \n",
      "\n",
      "\n",
      "======================================================================================\n",
      "원문\n",
      " Indian cricket team coach Ravi Shastri hoisted the Indian tricolour alongside captain Virat Kohli and the rest of the squad in a London hotel on the occasion of the 72nd Indian Independence Day. The team was seen standing in a formation around the flag, while Shastri and Kohli were seen standing together beside the hoisted tricolour. \n",
      "\n",
      "요약\n",
      "  \n",
      "\n",
      "\n",
      "======================================================================================\n",
      "원문\n",
      " Rohit Sharma became the first cricketer in ODI history to score three double hundreds after slamming 208*(153) against Sri Lanka today. Sharma slammed 12 sixes during his knock, taking his tally to 45 sixes in 2017, most for an Indian batsman in a calendar year. With his fifth 150+ score, Sharma also equalled record of most 150+ scores in ODIs.   \n",
      "\n",
      "요약\n",
      " Rohit Sharma became the first cricketer in ODI history to score three double hundreds after slamming 208*(153) against Sri Lanka today. \n",
      "\n",
      "\n",
      "======================================================================================\n"
     ]
    }
   ],
   "source": [
    "summarize_news([30, 400, 5060, 21454, 56645])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-chicago",
   "metadata": {},
   "source": [
    "요약의 품질도 우수하고, 문법적으로도 올바른 문장을 얻었습니다. 단점으로는 원문의 가장 짧은 문장보다 요약문이 짧아질 수는 없단 것입니다. 가장 중요한 문장을 골라내는 것이므로, 생성 모델에 비해 표현력은 매우 떨어지는 단점이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-uniform",
   "metadata": {},
   "source": [
    "# 회고 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-speaking",
   "metadata": {},
   "source": [
    "1️⃣ 잘한 점:\n",
    "\n",
    "불용어에서 의미를 추출해 낼 수 있는 단어는 제외하고, 여러 전처리 방법을 통해 추출적 요약에서 의미있는 결과를 얻었다.\n",
    "\n",
    "2️⃣ 어려웠던 점:\n",
    "\n",
    "inference 부분이 잘 이해가 가지 않는다. \n",
    "\n",
    "학습이 오래 걸려서 다양한 모델을 시도해 볼 수 없었다.\n",
    "\n",
    "\n",
    "3️⃣ 느낀 점:\n",
    "\n",
    "LSTM의 inference부분에 대해 글을 써 봐야 이해가 될 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
